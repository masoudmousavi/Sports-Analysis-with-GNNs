{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SP AZIZ-Result-Goal-BS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyhQYkmSdwkI",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9456a8ee-4e73-48a0-abb5-eba33088babe"
      },
      "source": [
        "#@title EXTENAL { form-width: \"15%\" }\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.9 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 407 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.2 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53btBN3VYJqa",
        "cellView": "form"
      },
      "source": [
        "#@title PRE-REQ { form-width: \"15%\" }\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.nn import Module,\\\n",
        "                     ModuleList,\\\n",
        "                     Embedding,\\\n",
        "                     BatchNorm1d,\\\n",
        "                     LogSoftmax,\\\n",
        "                     Softmax,\\\n",
        "                     Linear,\\\n",
        "                     NLLLoss,\\\n",
        "                     MSELoss,\\\n",
        "                     L1Loss,\\\n",
        "                     CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric as PyG\n",
        "from torch_geometric.data import Data, HeteroData\n",
        "from torch_geometric.nn.conv import RGCNConv, GINConv, GATConv, HeteroConv, GCNConv\n",
        "from torch_geometric.utils import to_networkx\n",
        "from collections import OrderedDict as od\n",
        "import logging\n",
        "import json\n",
        "from typing import NoReturn\n",
        "import typing\n",
        "import enum\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-5N1m4yYp4l",
        "cellView": "form"
      },
      "source": [
        "#@title UTILS { form-width: \"15%\" }\n",
        "class Globals(enum.Enum):\n",
        "    WON = 0\n",
        "    LOST_TO = 1\n",
        "    TIED_WITH = 2\n",
        "    PLAYED_IN = 3\n",
        "    USED = 4\n",
        "    BEFORE = 5\n",
        "    AFTER = 6\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "\n",
        "def stats(df: pd.DataFrame, show_players: bool=False, show_teams: bool=False, show_results: bool=False) -> NoReturn:\n",
        "  players_set = set()\n",
        "  players_list = list()\n",
        "  teams_set = set()\n",
        "\n",
        "  teams_list = list()\n",
        "  results = dict()\n",
        "  # index, (league, season, week, h_team, a_team, h_goal, a_goal, result, h_lineup, a_lineup, h_pass, a_pass)\n",
        "  for index, (league, season, week, h_team, a_team, h_goal, a_goal, result, h_lineup, a_lineup, h_pass, a_pass) in df.iterrows():\n",
        "    home_players, away_players = extract_players(h_lineup, a_lineup)\n",
        "    players_set.update(home_players + away_players)\n",
        "    players_list.extend(home_players + away_players)\n",
        "    if result == 'home':\n",
        "      results.update({f'{h_team} #Wins': results.get(f'{h_team} #Wins', 0)+1})\n",
        "      results.update({f'{a_team} #Losses': results.get(f'{a_team} #Losses', 0)+1})\n",
        "    elif result == 'tie':\n",
        "      results.update({f'{h_team} #Ties': results.get(f'{h_team} #Ties', 0)+1})\n",
        "      results.update({f'{a_team} #Ties': results.get(f'{a_team} #Ties', 0)+1})\n",
        "    else:\n",
        "      results.update({f'{a_team} #Wins': results.get(f'{a_team} #Wins', 0)+1})\n",
        "      results.update({f'{h_team} #Losses': results.get(f'{h_team} #Losses', 0)+1})\n",
        "\n",
        "    teams_list.extend([h_team, a_team])\n",
        "    teams_set.update([h_team, a_team])\n",
        "    \n",
        "  if show_players:\n",
        "    for player in players_set:\n",
        "      print(f'{player} played in {players_list.count(player)} matches.')\n",
        "  if show_teams:\n",
        "    for team in teams_set:\n",
        "      print(f'{team} played {teams_list.count(team)} matches.')\n",
        "  if show_results:\n",
        "    results = od(sorted(results.items()))\n",
        "    for key, val in results.items():\n",
        "      print(f'{key}: {val}')\n",
        "\n",
        "\n",
        "def home_result(result: str) -> int:\n",
        "  if result == 'win':\n",
        "    return Globals.WON.value\n",
        "  elif result == 'tie':\n",
        "    return Globals.TIED_WITH.value\n",
        "  elif result == 'loss':\n",
        "    return Globals.LOST_TO.value\n",
        "\n",
        "\n",
        "def remove_redundancy(players: list) -> list:\n",
        "  new_players = list()\n",
        "\n",
        "  for player in players:\n",
        "    if 'Own' in player:\n",
        "      player = player.replace('Own', '')\n",
        "    if 'Pen. Scored' in player:\n",
        "      player = player.replace('Pen. Scored', '')\n",
        "    if 'Pen. Score' in player:\n",
        "      player = player.replace('Pen. Score', '')\n",
        "    if 'Own' in player or 'Scored' in player or 'Score' in player:\n",
        "      print(player)\n",
        "      #SHOULD NOT PRINT IF CODE IS CORRECT\n",
        "    else:\n",
        "      new_players.append(player.strip())\n",
        "  return new_players\n",
        "\n",
        "\n",
        "def extract_players(home_lineup: str, away_lineup: str, seperator: str=' - ') -> list:\n",
        "  home_players = home_lineup[:].split(seperator)\n",
        "  away_players = away_lineup[:].split(seperator)\n",
        "  \n",
        "  return remove_redundancy(home_players), remove_redundancy(away_players)\n",
        "\n",
        "\n",
        "def extract_entities(df: pd.DataFrame) -> typing.Tuple[set, set]:\n",
        "  players_set = set()\n",
        "  players_list = list()\n",
        "  teams_set = set()\n",
        "\n",
        "  teams_list = list()\n",
        "  # results = dict()\n",
        "  for index, (league, season, week, h_team, a_team, h_goal, a_goal, result, h_lineup, a_lineup, h_pass, a_pass) in df.iterrows():\n",
        "    home_players, away_players = extract_players(h_lineup, a_lineup)\n",
        "\n",
        "    players_set.update(home_players + away_players)\n",
        "    teams_set.update([h_team, a_team])\n",
        "    \n",
        "  return teams_set, players_set\n",
        "\n",
        "\n",
        "def gen_entities(df: pd.DataFrame) -> dict:\n",
        "  teams, players = extract_entities(df)\n",
        "  entities = {entity: index for index, entity in enumerate(list(players) + list(teams))}\n",
        "  return entities\n",
        "\n",
        "\n",
        "def nodes_gen(df: pd.DataFrame) -> typing.Tuple[dict, dict]:\n",
        "  player_nodes = dict()\n",
        "  team_nodes = dict()\n",
        "  player_node_counter = 0\n",
        "  team_node_counter = 0\n",
        "\n",
        "  for index, (league, season, week, h_team, a_team, h_goal, a_goal, result, h_lineup, a_lineup, h_pass, a_pass) in df.iterrows():\n",
        "      home_players, away_players = extract_players(h_lineup, a_lineup)\n",
        "\n",
        "      for player_index, player in enumerate(home_players):\n",
        "        player_nodes[f'{player}@{index}'] = player_node_counter\n",
        "        player_node_counter += 1\n",
        "      for player_index, player in enumerate(away_players):\n",
        "        player_nodes[f'{player}@{index}'] = player_node_counter\n",
        "        player_node_counter += 1\n",
        "\n",
        "      team_nodes[f'{h_team}*{index}'] = team_node_counter\n",
        "      team_node_counter += 1\n",
        "\n",
        "      team_nodes[f'{a_team}*{index}'] = team_node_counter\n",
        "      team_node_counter += 1\n",
        "\n",
        "  return player_nodes, team_nodes\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmQxMLzZZDa0",
        "cellView": "form"
      },
      "source": [
        "#@title GNN { form-width: \"15%\" }\n",
        "#@title\n",
        "class HeteroGNN(Module):\n",
        "  def __init__(self, embedding_dims: tuple, conv_dims: list, fully_connected_dims: list, dropout: dict)-> NoReturn:\n",
        "    super(HeteroGNN, self).__init__()\n",
        "\n",
        "    self.mode = None # 'train' or 'test' or 'dev' later \n",
        "    self.output_dim = 3 #home_result: win, lose, tie\n",
        "    self.num_relations = 7 #win/lose/tie/play/use/after/before\n",
        "    self.dropout = dropout\n",
        "\n",
        "    #one-hot to latent\n",
        "    self.embed = Embedding(embedding_dims[0], embedding_dims[1])\n",
        "    \n",
        "    conv_list = [\n",
        "                  HeteroConv(\n",
        "                      {\n",
        "                          ('team', 'won', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'lost_to', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'tied_with', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('player', 'played_for', 'team'): GATConv(embedding_dims[-1], conv_dims[0], heads=1),\n",
        "                          ('team', 'used', 'player'): GATConv(embedding_dims[-1], conv_dims[0], heads=1),\n",
        "                          ('player', 'is_before', 'player'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('player', 'is_after', 'player'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'is_before', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'is_after', 'team'): GCNConv(embedding_dims[-1], conv_dims[0])\n",
        "                      }, aggr='sum'\n",
        "                  )\n",
        "                ] + \\\n",
        "                [\n",
        "                  HeteroConv(\n",
        "                      {\n",
        "                          ('team', 'won', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'lost_to', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'tied_with', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('player', 'played_for', 'team'): GATConv(conv_dims[i], conv_dims[i+1], heads=1),\n",
        "                          ('team', 'used', 'player'): GATConv(conv_dims[i], conv_dims[i+1], heads=1),\n",
        "                          ('player', 'is_before', 'player'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('player', 'is_after', 'player'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'is_before', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'is_after', 'team'): GCNConv(conv_dims[i], conv_dims[i+1])\n",
        "                      }, aggr='sum'\n",
        "                  )\n",
        "                  for i in range(len(conv_dims[:-1]))\n",
        "                ]\n",
        "\n",
        "\n",
        "              \n",
        "\n",
        "  \n",
        "    # batch_norm_list = [\n",
        "    #                      BatchNorm1d(conv_dims[i])\n",
        "    #                      for i in range(len(conv_dims[:-1]))\n",
        "    #                   ]\n",
        "\n",
        "    fully_connected_list =   [\n",
        "                                Linear(2*conv_dims[-1], fully_connected_dims[0])\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(fully_connected_dims[i], fully_connected_dims[i+1])\n",
        "                                for i in range(len(fully_connected_dims[:-1]))\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(fully_connected_dims[-1], self.output_dim)\n",
        "                             ]\n",
        "    #graph conv layers\n",
        "    self.conv_layers = ModuleList(conv_list)\n",
        "    #batch normalization layers\n",
        "\n",
        "    # self.batch_norm_layers = ModuleList(batch_norm_list)\n",
        "\n",
        "    #fully connected dense layers\n",
        "    self.fully_connected_layers = ModuleList(fully_connected_list)\n",
        "\n",
        "    self.classifier = LogSoftmax(dim=1)\n",
        "      \n",
        "\n",
        "  def reset_parameters(self):\n",
        "      self.embed.reset_parameters()\n",
        "      for conv in self.conv_layers:\n",
        "          # for layer in conv:\n",
        "          #   layer.reset_parameters()\n",
        "          conv.reset_parameters()\n",
        "      # for bn in self.batch_norm_layers:\n",
        "      #     bn.reset_parameters()\n",
        "      for fc in self.fully_connected_layers:\n",
        "          fc.reset_parameters()\n",
        "\n",
        "\n",
        "  def forward(self, data: HeteroData) -> torch.Tensor:\n",
        "    x_dict = data.x_dict\n",
        "    home_list = data.home_list\n",
        "    away_list = data.away_list\n",
        "\n",
        "    #print('1111', x_dict['player'][70180])\n",
        "\n",
        "    edge_index_dict = data.edge_index_dict\n",
        "    x_dict = {key: self.embed(x) for key, x in x_dict.items()}\n",
        "    \n",
        "    if self.training:\n",
        "      x_dict = {key: F.dropout(x, p=self.dropout[\"emb\"]) for key, x in x_dict.items()}\n",
        "\n",
        "    # for conv, bn in zip(self.conv_layers[:-1], self.batch_norm_layers):\n",
        "    for conv in self.conv_layers[:-1]:\n",
        "      x_dict = conv(x_dict, edge_index_dict=edge_index_dict)\n",
        "      x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
        "      if self.training:\n",
        "        x_dict = {key: F.dropout(x, p=self.dropout[\"conv\"]) for key, x in x_dict.items()}\n",
        "\n",
        "    x_dict = self.conv_layers[-1](x_dict, edge_index_dict=edge_index_dict)\n",
        "    #XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXxx\n",
        "\n",
        "    # print(x_dict['player'][data.home_p_list].shape)\n",
        "    self.h_p_dict = data.home_p_dict\n",
        "    self.h_p_strength = x_dict['player'][list(data.home_p_dict.values())]\n",
        "\n",
        "    #YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYy\n",
        "    \n",
        "    \n",
        "    \n",
        "    if self.training:\n",
        "      x_dict = {key: F.dropout(x, p=self.dropout[\"conv\"]) for key, x in x_dict.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##################################### End of Encoder \n",
        "    h = torch.cat(\n",
        "        (x_dict['team'][home_list], x_dict['team'][away_list]),\n",
        "        dim=1\n",
        "    )\n",
        "\n",
        "    for fc in self.fully_connected_layers[:-1]:\n",
        "      h = fc(h)\n",
        "      h = F.relu(h)\n",
        "      if self.training:\n",
        "        h = F.dropout(h, p=self.dropout[\"fc\"])\n",
        "\n",
        "    h = self.fully_connected_layers[-1](h)\n",
        "    # if self.training:\n",
        "    #   h = F.dropout(h, p=self.dropout[\"fc\"])\n",
        "\n",
        "    return self.classifier(h)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKbhUIvoW2M3"
      },
      "source": [
        "#@title GNN - With Box Score { form-width: \"15%\" }\n",
        "#@title\n",
        "class BSEXHeteroGNN(Module):\n",
        "  def __init__(self, embedding_dims: tuple, conv_dims: list, fully_connected_dims: list, pass_fully_connected_dims: list,  goal_fully_connected_dims: list, dropout: dict)-> NoReturn:\n",
        "    super(BSEXHeteroGNN, self).__init__()\n",
        "\n",
        "    self.mode = None # 'train' or 'test' or 'dev' later \n",
        "    self.output_dim = 3 #home_result: win, lose, tie\n",
        "    self.num_relations = 7 #win/lose/tie/play/use/after/before\n",
        "    self.dropout = dropout\n",
        "\n",
        "    #one-hot to latent\n",
        "    self.embed = Embedding(embedding_dims[0], embedding_dims[1])\n",
        "    \n",
        "    conv_list = [\n",
        "                  HeteroConv(\n",
        "                      {\n",
        "                          ('team', 'won', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'lost_to', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'tied_with', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('player', 'played_for', 'team'): GATConv(embedding_dims[-1], conv_dims[0], heads=1),\n",
        "                          ('team', 'used', 'player'): GATConv(embedding_dims[-1], conv_dims[0], heads=1),\n",
        "                          ('player', 'is_before', 'player'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('player', 'is_after', 'player'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'is_before', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'is_after', 'team'): GCNConv(embedding_dims[-1], conv_dims[0])\n",
        "                      }, aggr='sum'\n",
        "                  )\n",
        "                ] + \\\n",
        "                [\n",
        "                  HeteroConv(\n",
        "                      {\n",
        "                          ('team', 'won', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'lost_to', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'tied_with', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('player', 'played_for', 'team'): GATConv(conv_dims[i], conv_dims[i+1], heads=1),\n",
        "                          ('team', 'used', 'player'): GATConv(conv_dims[i], conv_dims[i+1], heads=1),\n",
        "                          ('player', 'is_before', 'player'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('player', 'is_after', 'player'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'is_before', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'is_after', 'team'): GCNConv(conv_dims[i], conv_dims[i+1])\n",
        "                      }, aggr='sum'\n",
        "                  )\n",
        "                  for i in range(len(conv_dims[:-1]))\n",
        "                ]\n",
        "\n",
        "\n",
        "              \n",
        "\n",
        "  \n",
        "    # batch_norm_list = [\n",
        "    #                      BatchNorm1d(conv_dims[i])\n",
        "    #                      for i in range(len(conv_dims[:-1]))\n",
        "    #                   ]\n",
        "\n",
        "    fully_connected_list =   [\n",
        "                                Linear(2*conv_dims[-1], fully_connected_dims[0])\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(fully_connected_dims[i], fully_connected_dims[i+1])\n",
        "                                for i in range(len(fully_connected_dims[:-1]))\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(fully_connected_dims[-1], self.output_dim)\n",
        "                             ]\n",
        "    \n",
        "\n",
        "    pass_fully_connected_list = [\n",
        "                                Linear(conv_dims[-1], pass_fully_connected_dims[0])\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(pass_fully_connected_dims[i], pass_fully_connected_dims[i+1])\n",
        "                                for i in range(len(pass_fully_connected_dims[:-1]))\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(pass_fully_connected_dims[-1], 1)\n",
        "                             ]\n",
        "\n",
        "    goal_fully_connected_list = [\n",
        "                                Linear(conv_dims[-1], goal_fully_connected_dims[0])\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(goal_fully_connected_dims[i], goal_fully_connected_dims[i+1])\n",
        "                                for i in range(len(goal_fully_connected_dims[:-1]))\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(goal_fully_connected_dims[-1], 1)\n",
        "                             ]\n",
        "\n",
        "\n",
        "\n",
        "    #graph conv layers\n",
        "    self.conv_layers = ModuleList(conv_list)\n",
        "    #batch normalization layers\n",
        "\n",
        "    # self.batch_norm_layers = ModuleList(batch_norm_list)\n",
        "\n",
        "    #fully connected dense layers\n",
        "    self.fully_connected_layers = ModuleList(fully_connected_list)\n",
        "    self.pass_fully_connected_layers = ModuleList(pass_fully_connected_list)\n",
        "    self.goal_fully_connected_layers = ModuleList(goal_fully_connected_list)\n",
        "\n",
        "    self.classifier = LogSoftmax(dim=1)\n",
        "      \n",
        "\n",
        "  def reset_parameters(self):\n",
        "      self.embed.reset_parameters()\n",
        "      for conv in self.conv_layers:\n",
        "          # for layer in conv:\n",
        "          #   layer.reset_parameters()\n",
        "          conv.reset_parameters()\n",
        "      # for bn in self.batch_norm_layers:\n",
        "      #     bn.reset_parameters()\n",
        "      for fc in self.fully_connected_layers:\n",
        "          fc.reset_parameters()\n",
        "\n",
        "\n",
        "  def forward(self, data: HeteroData) -> torch.Tensor:\n",
        "    x_dict = data.x_dict\n",
        "    home_list = data.home_list\n",
        "    away_list = data.away_list\n",
        "    home_p_list = data.home_p_list\n",
        "    away_p_list = data.away_p_list\n",
        "\n",
        "    #print('1111', x_dict['player'][70180])\n",
        "\n",
        "    edge_index_dict = data.edge_index_dict\n",
        "    x_dict = {key: self.embed(x) for key, x in x_dict.items()}\n",
        "    \n",
        "    if self.training:\n",
        "      x_dict = {key: F.dropout(x, p=self.dropout[\"emb\"]) for key, x in x_dict.items()}\n",
        "\n",
        "    # for conv, bn in zip(self.conv_layers[:-1], self.batch_norm_layers):\n",
        "    for conv in self.conv_layers[:-1]:\n",
        "      x_dict = conv(x_dict, edge_index_dict=edge_index_dict)\n",
        "      x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
        "      if self.training:\n",
        "        x_dict = {key: F.dropout(x, p=self.dropout[\"conv\"]) for key, x in x_dict.items()}\n",
        "\n",
        "    x_dict = self.conv_layers[-1](x_dict, edge_index_dict=edge_index_dict)\n",
        "    #XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXxx\n",
        "\n",
        "    # print(x_dict['player'][data.home_p_list].shape)\n",
        "    self.h_p_dict = data.home_p_dict\n",
        "    self.h_p_strength = x_dict['player'][list(data.home_p_dict.values())]\n",
        "\n",
        "    #YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYy\n",
        "    \n",
        "    \n",
        "    \n",
        "    if self.training:\n",
        "      x_dict = {key: F.dropout(x, p=self.dropout[\"conv\"]) for key, x in x_dict.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##################################### End of Encoder \n",
        "    h = torch.cat(\n",
        "        (x_dict['team'][home_list], x_dict['team'][away_list]),\n",
        "        dim=1\n",
        "    )\n",
        "    \n",
        "    for fc in self.fully_connected_layers[:-1]:\n",
        "      h = fc(h)\n",
        "      h = F.relu(h)\n",
        "      if self.training:\n",
        "        h = F.dropout(h, p=self.dropout[\"fc\"])\n",
        "\n",
        "    h = self.fully_connected_layers[-1](h)\n",
        "    if self.training:\n",
        "      h = F.dropout(h, p=self.dropout[\"fc\"])\n",
        "    goal_h = torch.cat(\n",
        "        (x_dict['team'][home_list], x_dict['team'][away_list])\n",
        "    )\n",
        "\n",
        "    for fc in self.goal_fully_connected_layers[:-1]:\n",
        "      goal_h = fc(goal_h)\n",
        "      goal_h = F.relu(goal_h)\n",
        "      if self.training:\n",
        "        goal_h = F.dropout(goal_h, p=self.dropout[\"fc\"])\n",
        "\n",
        "    goal_h = self.goal_fully_connected_layers[-1](goal_h)\n",
        "    if self.training:\n",
        "      goal_h = F.dropout(goal_h, p=self.dropout['fc'])\n",
        "\n",
        "    pass_h = torch.cat(\n",
        "        (x_dict['player'][home_p_list], x_dict['player'][away_p_list]),\n",
        "        dim=1\n",
        "    )\n",
        "\n",
        "    for fc in self.pass_fully_connected_layers[:-1]:\n",
        "      pass_h = fc(pass_h)\n",
        "      pass_h = F.relu(pass_h)\n",
        "      if self.training:\n",
        "        pass_h = F.dropout(pass_h, p=self.dropout[\"fc\"])\n",
        "\n",
        "    pass_h = self.pass_fully_connected_layers[-1](pass_h)\n",
        "    if self.training:\n",
        "      pass_h = F.dropout(pass_h, p=self.dropout['fc'])\n",
        "\n",
        "    return self.classifier(h), goal_h.reshape(-1), pass_h.reshape((len(home_p_list + away_p_list), -1))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVH9MBoUZM9b",
        "cellView": "form"
      },
      "source": [
        "#@title Graph { form-width: \"15%\" }\n",
        "#@title\n",
        "def home_won_gen(df: pd.DataFrame, full_data_frame=None) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "\n",
        "  home_winning_matches = df.loc[df['result'] == 'home']\n",
        "  home_winners = home_winning_matches['home_team']\n",
        "  away_losers = home_winning_matches['away_team']\n",
        "\n",
        "  winning_hashes = list()\n",
        "  losing_hashes = list()\n",
        "\n",
        "  for home, away, match in zip(home_winners, away_losers, home_winners.index):\n",
        "    winning_hashes.append(f'{home}*{match}')\n",
        "    losing_hashes.append(f'{away}*{match}')\n",
        "\n",
        "  winning_nodes = list()\n",
        "  losing_nodes = list()\n",
        "\n",
        "  if full_data_frame is None:\n",
        "    full_data_frame = df\n",
        "  _, team_nodes = nodes_gen(full_data_frame)\n",
        "\n",
        "  for winner, loser in zip(winning_hashes, losing_hashes):\n",
        "    winning_nodes.append(team_nodes[winner]) \n",
        "    losing_nodes.append(team_nodes[loser])\n",
        "\n",
        "  won_edges = torch.tensor(\n",
        "      [\n",
        "      winning_nodes,\n",
        "      losing_nodes\n",
        "      ], \n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  lost_edges = torch.tensor(\n",
        "      [\n",
        "      losing_nodes,\n",
        "      winning_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  return won_edges, lost_edges\n",
        "\n",
        "\n",
        "def away_won_gen(df: pd.DataFrame, full_data_frame=None) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  away_winning_matches = df.loc[df['result'] == 'away']\n",
        "  away_winners = away_winning_matches['away_team']\n",
        "  home_losers = away_winning_matches['home_team']\n",
        "\n",
        "  winning_hashes = list()\n",
        "  losing_hashes = list()\n",
        "\n",
        "  for home, away, match in zip(home_losers, away_winners, away_winners.index):\n",
        "    winning_hashes.append(f'{away}*{match}')\n",
        "    losing_hashes.append(f'{home}*{match}')\n",
        "\n",
        "  winning_nodes = list()\n",
        "  losing_nodes = list()\n",
        "\n",
        "  if full_data_frame is None:\n",
        "    full_data_frame = df\n",
        "  _, team_nodes = nodes_gen(full_data_frame)\n",
        "\n",
        "  for winner, loser in zip(winning_hashes, losing_hashes):\n",
        "    winning_nodes.append(team_nodes[winner]) \n",
        "    losing_nodes.append(team_nodes[loser])\n",
        "\n",
        "  won_edges = torch.tensor(\n",
        "      [\n",
        "      winning_nodes,\n",
        "      losing_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  lost_edges = torch.tensor(\n",
        "      [\n",
        "      losing_nodes,\n",
        "      winning_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "  \n",
        "  return won_edges, lost_edges\n",
        "\n",
        "\n",
        "def tied_gen(df: pd.DataFrame, full_data_frame=None) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  tied_matches = df.loc[df['result'] == 'tie']\n",
        "  home_teams = tied_matches['home_team']\n",
        "  away_teams = tied_matches['away_team']\n",
        "\n",
        "  home_hashes = list()\n",
        "  away_hashes = list()\n",
        "\n",
        "  for home, away, match in zip(home_teams, away_teams, away_teams.index):\n",
        "    away_hashes.append(f'{away}*{match}')\n",
        "    home_hashes.append(f'{home}*{match}')\n",
        "\n",
        "  home_nodes = list()\n",
        "  away_nodes = list()\n",
        "\n",
        "  if full_data_frame is None:\n",
        "    full_data_frame = df\n",
        "  _, team_nodes = nodes_gen(full_data_frame)\n",
        "\n",
        "  for home, away in zip(home_hashes, away_hashes):\n",
        "    home_nodes.append(team_nodes[home]) \n",
        "    away_nodes.append(team_nodes[away])\n",
        "\n",
        "  home_tied_edges = torch.tensor(\n",
        "      [\n",
        "      home_nodes,\n",
        "      away_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  away_tied_edges = torch.tensor(\n",
        "      [\n",
        "      away_nodes,\n",
        "      home_nodes\n",
        "      ], \n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  return home_tied_edges, away_tied_edges\n",
        "\n",
        "\n",
        "def played_used_gen(df: pd.DataFrame) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  team_nodes = list()\n",
        "  player_nodes = list()\n",
        "\n",
        "  p_nodes, t_nodes = nodes_gen(df)\n",
        "\n",
        "  for index, (league, season, week, h_team, a_team, h_goal, a_goal, result, h_lineup, a_lineup, h_pass, a_pass) in df.iterrows():\n",
        "    home_players, away_players = extract_players(h_lineup, a_lineup)\n",
        "\n",
        "    for home_player, away_player in zip(home_players, away_players):\n",
        "      player_nodes.append(p_nodes[f'{home_player}@{index}'])\n",
        "      team_nodes.append(t_nodes[f'{h_team}*{index}'])\n",
        "      player_nodes.append(p_nodes[f'{away_player}@{index}'])\n",
        "      team_nodes.append(t_nodes[f'{a_team}*{index}'])\n",
        "\n",
        "  played_in_edges = torch.tensor(\n",
        "      [\n",
        "       player_nodes,\n",
        "       team_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  used_edges = torch.tensor(\n",
        "      [\n",
        "       team_nodes,\n",
        "       player_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  ) \n",
        "\n",
        "  return played_in_edges, used_edges\n",
        "\n",
        "\n",
        "def players_before_after_gen(df: pd.DataFrame) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  player_match_hashes = list()\n",
        "\n",
        "  for index, (league, season, week, h_team, a_team, h_goal, a_goal, result, h_lineup, a_lineup, h_pass, a_pass) in df.iterrows():\n",
        "      home_players, away_players = extract_players(h_lineup, a_lineup)\n",
        "\n",
        "      for player in home_players + away_players:\n",
        "        player_match_hashes.append(f'{player}@{index}')\n",
        "\n",
        "\n",
        "\n",
        "  sorted_hashes = sorted(\n",
        "      player_match_hashes,\n",
        "      key=lambda w: (w.split('@')[0], int(w.split('@')[1]))\n",
        "  )\n",
        "\n",
        "  before_nodes = list()\n",
        "  after_nodes = list()\n",
        "\n",
        "  player_nodes, _ = nodes_gen(df)\n",
        "\n",
        "  for index, hash in enumerate(sorted_hashes):\n",
        "    player, match = hash.split('@')\n",
        "    before_node = player_nodes[hash]\n",
        "    try:\n",
        "      after_node = player_nodes[sorted_hashes[index+1]]\n",
        "      before_name = player_match_hashes[before_node].split('@')[0]\n",
        "      after_name = player_match_hashes[after_node].split('@')[0]\n",
        "      if before_name == after_name:\n",
        "        before_nodes.append(before_node)\n",
        "        after_nodes.append(after_node)\n",
        "    except:\n",
        "      pass\n",
        "  before_edges = torch.tensor(\n",
        "      [\n",
        "      before_nodes,\n",
        "      after_nodes\n",
        "      ], dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  after_edges = torch.tensor(\n",
        "      [\n",
        "      after_nodes,\n",
        "      before_nodes\n",
        "      ], dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  return before_edges, after_edges\n",
        "\n",
        "\n",
        "def teams_before_after_gen(df: pd.DataFrame) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  team_match_hashes = list()\n",
        "\n",
        "  for index, (league, season, week, h_team, a_team, h_goal, a_goal, result, h_lineup, a_lineup, h_pass, a_pass) in df.iterrows():\n",
        "      team_match_hashes.append(f'{h_team}*{index}')\n",
        "      team_match_hashes.append(f'{a_team}*{index}')\n",
        "\n",
        "  sorted_hashes = sorted(\n",
        "      team_match_hashes,\n",
        "      key= lambda w: (w.split('*')[0], int(w.split('*')[1]))\n",
        "  )\n",
        "\n",
        "  before_nodes = list()\n",
        "  after_nodes = list()\n",
        "\n",
        "  _, team_nodes = nodes_gen(df)\n",
        "\n",
        "  for index, hash in enumerate(sorted_hashes):\n",
        "    team, match = hash.split('*')\n",
        "    before_node = team_nodes[hash]\n",
        "    try:\n",
        "      after_node = team_nodes[sorted_hashes[index+1]]\n",
        "      before_name = team_match_hashes[before_node].split('*')[0]\n",
        "      after_name = team_match_hashes[after_node].split('*')[0]\n",
        "      if before_name == after_name:\n",
        "        before_nodes.append(before_node)\n",
        "        after_nodes.append(after_node)\n",
        "    except:\n",
        "      pass\n",
        "  before_edges = torch.tensor(\n",
        "      [\n",
        "      before_nodes,\n",
        "      after_nodes\n",
        "      ], dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  after_edges = torch.tensor(\n",
        "      [\n",
        "      after_nodes,\n",
        "      before_nodes\n",
        "      ], dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  return before_edges, after_edges\n",
        "\n",
        "\n",
        "def complete_graph_gen(df: pd.DataFrame, for_players: bool=True, for_teams: bool=True) -> dict:\n",
        "  home_won, away_lost = home_won_gen(df)\n",
        "  away_won, home_lost = away_won_gen(df)\n",
        "  home_tied, away_tied = tied_gen(df)\n",
        "  player_played, team_used = played_used_gen(df)\n",
        "\n",
        "  if for_players:\n",
        "    player_before, player_after = players_before_after_gen(df)\n",
        "  if for_teams:\n",
        "    team_before, team_after = teams_before_after_gen(df)\n",
        "  won_edge_index = torch.cat(\n",
        "      (home_won, away_won),\n",
        "      dim=1\n",
        "  )\n",
        "  lost_edge_index = torch.cat(\n",
        "      (away_lost, home_lost),\n",
        "      dim=1\n",
        "  )\n",
        "  tied_edge_index = torch.cat(\n",
        "      (home_tied, away_tied),\n",
        "      dim=1\n",
        "  )\n",
        "  edge_index = {\n",
        "      'won': won_edge_index,\n",
        "      'lost': lost_edge_index,\n",
        "      'tied': tied_edge_index,\n",
        "      'played': player_played,\n",
        "      'used': team_used,\n",
        "      'p_after':player_after,\n",
        "      'p_before': player_before,\n",
        "      't_after': team_after,\n",
        "      't_before': team_after\n",
        "  }   \n",
        "  return edge_index\n",
        "\n",
        "\n",
        "def supervision_graph_gen(df : pd.DataFrame, messaging: list, supervision: list, for_players: bool=True, for_teams: bool=True, log_supervision_matches: bool=False) -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
        "    #   if log_supervision_matches:\n",
        "    #     if model.mode == 'train':\n",
        "    #       mode = 'training'\n",
        "    #     elif model.mode == 'dev':\n",
        "    #       mode = 'validating'\n",
        "    #     elif model.mode == 'test':\n",
        "    #       mode = 'testing'\n",
        "    #     logging.info(\n",
        "    #         f'Messaging on matches ({messaging[0] + 1} -> {messaging[-1] + 1:>5}),\\ Model is {mode} on matches ({last_match+2} -> {last_match + 11})'\n",
        "    #     )\n",
        "  target_for_nodes = df\n",
        "\n",
        "  home_won, away_lost = home_won_gen(df.loc[messaging], full_data_frame=target_for_nodes)\n",
        "  away_won, home_lost = away_won_gen(df.loc[messaging], full_data_frame=target_for_nodes)\n",
        "  home_tied, away_tied = tied_gen(df.loc[messaging], full_data_frame=target_for_nodes)\n",
        "\n",
        "  player_played, team_used = played_used_gen(df)\n",
        "\n",
        "  if for_players:\n",
        "    player_before, player_after = players_before_after_gen(df)\n",
        "  if for_teams:\n",
        "    team_before, team_after = teams_before_after_gen(df)\n",
        "\n",
        "  won_edge_index = torch.cat(\n",
        "      (home_won, away_won),\n",
        "      dim=1\n",
        "  )\n",
        "  lost_edge_index = torch.cat(\n",
        "      (away_lost, home_lost),\n",
        "      dim=1\n",
        "  )\n",
        "  tied_edge_index = torch.cat(\n",
        "      (home_tied, away_tied),\n",
        "      dim=1\n",
        "  )\n",
        "  edge_index = {\n",
        "      'won': won_edge_index,\n",
        "      'lost': lost_edge_index,\n",
        "      'tied': tied_edge_index,\n",
        "      'played': player_played,\n",
        "      'used': team_used,\n",
        "      'p_after':player_after,\n",
        "      'p_before': player_before,\n",
        "      't_after': team_after,\n",
        "      't_before': team_after\n",
        "  }  \n",
        "  return edge_index\n",
        "\n",
        "\n",
        "def data_gen(df: pd.DataFrame, messaging: list, supervision: list=None, remove_supervision_links: bool=True, for_players: bool=True, for_teams: bool=True, print_edges: bool=False, log_supervision_matches: bool=False) -> HeteroData:\n",
        "    #   if print_edges:\n",
        "    #     show_edges(df, edge_index, edge_type)\n",
        "  if remove_supervision_links:\n",
        "    edge_index = supervision_graph_gen(\n",
        "        df,\n",
        "        messaging=messaging,\n",
        "        supervision=supervision,\n",
        "        for_players=for_players,\n",
        "        for_teams=for_teams,\n",
        "        log_supervision_matches=log_supervision_matches\n",
        "    )\n",
        "    y = torch.tensor(\n",
        "        df.loc[supervision]['result'].map(home_result).values,\n",
        "        device=Globals.DEVICE.value\n",
        "    )\n",
        "\n",
        "  else:\n",
        "    if supervision is None:\n",
        "      supervision = df.index\n",
        "    if messaging is None:\n",
        "      messaging = df.index\n",
        "    edge_index = complete_graph_gen(df, for_players, for_teams)\n",
        "    y = torch.tensor(\n",
        "        df.loc[supervision]['result'].map(home_result).values,\n",
        "        device=Globals.DEVICE.value\n",
        "    )\n",
        "\n",
        "  data = HeteroData()\n",
        "  data['player'].x = torch.unique(edge_index['played'][0]).to(Globals.DEVICE.value).type(torch.int64)\n",
        "  data['team'].x = torch.unique(edge_index['used'][0]).to(Globals.DEVICE.value).type(torch.int64)\n",
        "  \n",
        "  data['team', 'won', 'team'].edge_index = edge_index['won']\n",
        "  data['team', 'lost_to', 'team'].edge_index = edge_index['lost']\n",
        "  data['team', 'tied_with', 'team'].edge_index = edge_index['tied']\n",
        "  data['player', 'played_for', 'team'].edge_index = edge_index['played']\n",
        "  data['team', 'used', 'player'].edge_index = edge_index['used']\n",
        "  data['player', 'is_before', 'player'].edge_index = edge_index['p_before']\n",
        "  data['player', 'is_after', 'player'].edge_index = edge_index['p_after']\n",
        "  data['team', 'is_before', 'team'].edge_index = edge_index['t_before']\n",
        "  data['team', 'is_after', 'team'].edge_index = edge_index['t_after']\n",
        "  data.y = y\n",
        "\n",
        "\n",
        "  data.y_pass = torch.cat((\n",
        "    torch.tensor(np.stack(df.loc[supervision, 'home_passes'].apply(lambda z: list(map(int, z.split(' - ')))))),\n",
        "    torch.tensor(np.stack(df.loc[supervision, 'away_passes'].apply(lambda z: list(map(int, z.split(' - '))))))\n",
        "    ), dim=1).to(Globals.DEVICE.value).float()\n",
        "\n",
        "  data.y_goal = torch.cat(\n",
        "      (\n",
        "          torch.tensor(df.loc[supervision]['home_goal'].values),\n",
        "          torch.tensor(df.loc[supervision]['away_goal'].values)\n",
        "      )\n",
        "  ).to(Globals.DEVICE.value).type(torch.float)\n",
        "  return data\n",
        "\n",
        "\n",
        "def batch_gen(df: pd.DataFrame, entities: dict, messaging: list=None, supervision: list=None, remove_supervision_links: bool=True, log_supervision_matches: bool=False) -> HeteroData:\n",
        "  graph = data_gen(\n",
        "      df,\n",
        "      messaging=messaging,\n",
        "      supervision=supervision, \n",
        "      remove_supervision_links=remove_supervision_links,\n",
        "      log_supervision_matches=log_supervision_matches\n",
        "  )\n",
        "  \n",
        "  home_teams = list()\n",
        "  away_teams = list()\n",
        "\n",
        "  p_nodes, t_nodes = nodes_gen(df)\n",
        "  nodes = {**p_nodes, **t_nodes}\n",
        "  \n",
        "  if supervision is None:\n",
        "    supervision = df.index\n",
        "\n",
        "  indices = dict()\n",
        "  for hash, index in nodes.items():\n",
        "    if '@' in hash:\n",
        "      player = hash.split('@')[0]\n",
        "      player_id = entities[player]\n",
        "      indices.update({index:player_id})\n",
        "    elif '*' in hash:\n",
        "      team = hash.split('*')[0]\n",
        "      team_id = entities[team]\n",
        "      indices.update({index: team_id})\n",
        "  home_players = list()\n",
        "  away_players = list()\n",
        "\n",
        "\n",
        "  graph.home_p_dict = dict()\n",
        "  for index, (league, season, week, h_team, a_team, h_goal, a_goal, result, h_lineup, a_lineup, h_pass, a_pass) in df.loc[supervision].iterrows():\n",
        "      home_teams.append(nodes[f'{h_team}*{index}'])\n",
        "      away_teams.append(nodes[f'{a_team}*{index}'])\n",
        "      h_players, a_players = extract_players(h_lineup, a_lineup)\n",
        "      h_players = list(map(lambda s: s+f'@{index}', h_players))\n",
        "      a_players = list(map(lambda s: s+f'@{index}', a_players))\n",
        "      home_players.append(list(map(lambda s: nodes[s], h_players)))\n",
        "      away_players.append(list(map(lambda s: nodes[s], a_players)))\n",
        "\n",
        "      graph.home_p_dict.update(dict(zip(h_players, list(map(lambda s: nodes[s], h_players)))))\n",
        "\n",
        "\n",
        "      # print(h_players)\n",
        "      # print('-'*20)\n",
        "      # print(list(map(lambda s: nodes[s], h_players)))\n",
        "      # print('='*60)\n",
        "\n",
        "\n",
        "      \n",
        "  features_player = torch.tensor(\n",
        "      [indices[i.item()] for i in graph['player'].x],\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "  features_team = torch.tensor(\n",
        "      [indices[i.item()] for i in graph['team'].x],\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  graph['player'].x = features_player\n",
        "  graph['team'].x = features_team\n",
        "  graph.home_list = home_teams\n",
        "  graph.away_list = away_teams\n",
        "  graph.home_p_list = torch.tensor(home_players).to(Globals.DEVICE.value)\n",
        "  graph.away_p_list = torch.tensor(away_players).to(Globals.DEVICE.value)\n",
        "  \n",
        "  return graph\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVHmd0EWZkqD",
        "cellView": "form"
      },
      "source": [
        "#@title Learning { form-width: \"15%\" }\n",
        "#@title\n",
        "def train(model: BSEXHeteroGNN, data: HeteroData, optimizer: torch.optim, result_loss_fn: torch.nn.modules.loss, goal_loss_fn: torch.nn.modules.loss, BS_loss_fn: torch.nn.modules.loss) -> typing.Tuple[float, int, int]:\n",
        "  batch_loss = 0\n",
        "\n",
        "  model.train()\n",
        "  result_out, goal_out, BS_out = model(data)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss_result = result_loss_fn(result_out, data.y)\n",
        "  loss_BS = BS_loss_fn(BS_out, data.y_pass) / 10\n",
        "  loss_goal = goal_loss_fn(goal_out, data.y_goal)\n",
        "  loss = loss_result + loss_BS + loss_goal\n",
        "  batch_loss = loss.item()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  prediction = result_out.argmax(dim=-1)\n",
        "  correct = (prediction == data.y).sum().item()\n",
        "  all = data.y.shape[0]\n",
        "\n",
        "  return batch_loss, loss_result.item(), loss_goal.item(), loss_BS.item(), correct, all\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: HeteroGNN, data: HeteroData, goal_loss_fn, BS_loss_fn) -> typing.Tuple[int, int]:\n",
        "  model.eval()\n",
        "\n",
        "  # for child in model.children():\n",
        "  #   for ii in range(len(child)):\n",
        "  #       if type(child[ii]) == BatchNorm1d:\n",
        "  #           child[ii].track_running_stats = False\n",
        "\n",
        "  result_out, goal_out, BS_out = model(data)\n",
        "  bs_loss = BS_loss_fn(BS_out, data.y_pass)\n",
        "  goal_loss = goal_loss_fn(goal_out, data.y_goal)\n",
        "  prediction = result_out.argmax(dim=-1)\n",
        "  correct = (prediction == data.y).sum().item()\n",
        "  all = data.y.shape[0]\n",
        "  model.train()\n",
        "\n",
        "  return correct, all, goal_loss.item(), bs_loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7vo4ykoZrlS",
        "cellView": "form",
        "outputId": "7e4a1a91-e04d-4179-f2cb-e39c827facc1"
      },
      "source": [
        "#@title MAIN { form-width: \"15%\" }\n",
        "#@title\n",
        "dataset_filename = 'FakeData_EPL.csv'\n",
        "\n",
        "hp_file = open('hyperparameters.json', 'w')\n",
        "hyperparameters = {\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"num_epochs\": 200,\n",
        "    \"fc_dropout\":0.03,\n",
        "    \"conv_dropout\": 0.02,\n",
        "    \"emb_dropout\": 0.01,\n",
        "    \"train_messaging_graph_size\": 440,\n",
        "    \"val_messaging_graph_size\": 440,\n",
        "    \"test_messaging_graph_size\": 440,\n",
        "    \"iter_size\": 10,\n",
        "    \"val_week_denom\": 50,\n",
        "    \"test_week_denom\": 60,\n",
        "    \"embedding_dim\": 2,\n",
        "    \"conv_dims\":[\n",
        "          2,\n",
        "          2, \n",
        "          2\n",
        "    ],\n",
        "    \"fully_connected_dims\":[\n",
        "              2,\n",
        "              2\n",
        "    ]\n",
        "}\n",
        "\n",
        "json.dump(hyperparameters, hp_file, indent= 4)\n",
        "hp_file.close()\n",
        "\n",
        "dataset = pd.read_csv(\n",
        "    dataset_filename,\n",
        "    encoding='latin-1',\n",
        "    usecols=['league', 'season', 'week', 'home_team', 'away_team', 'result', 'home_lineup', 'away_lineup', 'home_passes', 'away_passes', 'home_goal', 'away_goal']\n",
        ")\n",
        "corrupted = dataset.loc[pd.isna(dataset['away_lineup']) | pd.isna(dataset['home_lineup'])]\n",
        "dataset = dataset.drop(corrupted.index, axis=0).reset_index(drop=True)\n",
        "\n",
        "\n",
        "with open('hyperparameters.json', 'r') as hp_file:\n",
        "    hyperparameters = json.load(hp_file)\n",
        "\n",
        "learning_rate = hyperparameters[\"learning_rate\"]\n",
        "num_epochs = hyperparameters[\"num_epochs\"]\n",
        "fc_dropout = hyperparameters[\"fc_dropout\"]\n",
        "conv_dropout = hyperparameters[\"conv_dropout\"]\n",
        "emb_dropout = hyperparameters[\"emb_dropout\"]\n",
        "\n",
        "entities = gen_entities(dataset)\n",
        "\n",
        "model = BSEXHeteroGNN(\n",
        "    embedding_dims=(\n",
        "        max(entities.values()) + 1,\n",
        "        hyperparameters[\"embedding_dim\"]\n",
        "    ),\n",
        "    conv_dims=hyperparameters[\"conv_dims\"],\n",
        "    fully_connected_dims=hyperparameters[\"fully_connected_dims\"],\n",
        "    pass_fully_connected_dims=hyperparameters[\"fully_connected_dims\"],\n",
        "    goal_fully_connected_dims=hyperparameters[\"fully_connected_dims\"],\n",
        "    dropout={\n",
        "        \"emb\": emb_dropout,\n",
        "        \"conv\": conv_dropout,\n",
        "        \"fc\": fc_dropout\n",
        "    }\n",
        ").to(Globals.DEVICE.value)\n",
        "\n",
        "print(model)\n",
        "\n",
        "optimizer = Adam(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate\n",
        ")\n",
        "criterion_result = NLLLoss()\n",
        "criterion_BS = L1Loss()\n",
        "criterion_goal = MSELoss()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BSEXHeteroGNN(\n",
            "  (embed): Embedding(300, 2)\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): HeteroConv(num_relations=9)\n",
            "    (1): HeteroConv(num_relations=9)\n",
            "    (2): HeteroConv(num_relations=9)\n",
            "  )\n",
            "  (fully_connected_layers): ModuleList(\n",
            "    (0): Linear(in_features=4, out_features=2, bias=True)\n",
            "    (1): Linear(in_features=2, out_features=2, bias=True)\n",
            "    (2): Linear(in_features=2, out_features=3, bias=True)\n",
            "  )\n",
            "  (pass_fully_connected_layers): ModuleList(\n",
            "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
            "    (1): Linear(in_features=2, out_features=2, bias=True)\n",
            "    (2): Linear(in_features=2, out_features=1, bias=True)\n",
            "  )\n",
            "  (goal_fully_connected_layers): ModuleList(\n",
            "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
            "    (1): Linear(in_features=2, out_features=2, bias=True)\n",
            "    (2): Linear(in_features=2, out_features=1, bias=True)\n",
            "  )\n",
            "  (classifier): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlwyfhEhtGzv",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668d8785-7663-48e0-ae63-fe5179393da8"
      },
      "source": [
        "#@title DATASET SPLIT 2 { form-width: \"5%\" }\n",
        "validation_indcs = dataset.sample(n=300).index.values.reshape(-1)\n",
        "_dataset = dataset.drop(validation_indcs)\n",
        "\n",
        "test_indcs = _dataset.sample(n=300).index.values\n",
        "_dataset = _dataset.drop(test_indcs)\n",
        "\n",
        "val_graph_indcs = np.concatenate(\n",
        "    (_dataset.index.values, validation_indcs),\n",
        "    axis=0\n",
        ")\n",
        "hd_val = batch_gen(\n",
        "    df=dataset.loc[val_graph_indcs],\n",
        "    entities=entities,\n",
        "    remove_supervision_links=True,\n",
        "    messaging=_dataset.index,\n",
        "    supervision=validation_indcs\n",
        ")\n",
        "test_graph_indcs = np.concatenate(\n",
        "    (_dataset.index.values, validation_indcs, test_indcs),\n",
        "    axis=0\n",
        ")\n",
        "hd_test = batch_gen(\n",
        "    df=dataset.loc[test_graph_indcs],\n",
        "    entities=entities,\n",
        "    remove_supervision_links=True,\n",
        "    messaging=np.concatenate((_dataset.index.values, validation_indcs), axis=0),\n",
        "    supervision=test_indcs\n",
        ")\n",
        "\n",
        "\n",
        "hd_train = list()\n",
        "\n",
        "try:\n",
        "    for i in tqdm(range(0, _dataset.shape[0], 10)):\n",
        "        train_supervision_indcs = _dataset.loc[_dataset.index[i]: _dataset.index[i + 9], :].index\n",
        "        train_messaging_indcs = _dataset.drop(train_supervision_indcs).index\n",
        "        hd = batch_gen(\n",
        "            df=_dataset,\n",
        "            entities=entities,\n",
        "            remove_supervision_links=True,\n",
        "            messaging=train_messaging_indcs,\n",
        "            supervision=train_supervision_indcs\n",
        "        )\n",
        "        hd_train.append(hd)\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 5/320 [00:33<35:27,  6.75s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okunOzHgW2M_",
        "outputId": "5a273973-6306-4269-8d1f-069af933dd3c"
      },
      "source": [
        "model.embed(torch.tensor([287]).to(Globals.DEVICE.value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2.3904, 0.2990]], device='cuda:0', grad_fn=<EmbeddingBackward>)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRme7zAHW2M_"
      },
      "source": [
        "model.reset_parameters()\n",
        "home_p_dict_list = []\n",
        "home_p_strength_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iLowqR_e6YB",
        "cellView": "form"
      },
      "source": [
        "#@title Train/Val/Test { form-width: \"15%\" }\n",
        "try:\n",
        "  for epoch in range(150):\n",
        "    all = 0\n",
        "    correct = 0\n",
        "    loss = 0\n",
        "    result_loss = 0\n",
        "    goal_loss = 0\n",
        "    BS_loss = 0\n",
        "    for hd in hd_train:\n",
        "      out_loss, out_rl, out_gl, out_BSl, out_c, out_a = train(model, hd, optimizer, criterion_result, criterion_goal, criterion_BS)\n",
        "      loss += out_loss\n",
        "      result_loss += out_rl\n",
        "      goal_loss += out_gl\n",
        "      BS_loss += out_BSl\n",
        "      correct += out_c\n",
        "      all += out_a\n",
        "    print(f'======================================= EPOCH {epoch + 1} ===================================')\n",
        "    print(f'Loss: {loss}, Train Acc: {correct / all: .3f}')\n",
        "    print(f'Result Loss: {result_loss}, BoxScore Loss: {BS_loss}, Goal Loss: {goal_loss}')\n",
        "    # home_p_dict_list.append(model.h_p_dict)\n",
        "    # home_p_strength_list.append(model.h_p_strength.detach().cpu())\n",
        "    \n",
        "    correct, all, goal_loss, BS_loss = evaluate(model, hd_val, criterion_goal, criterion_BS)\n",
        "    print(f'Dev Acc: {correct / all: .3f}')\n",
        "    print(f'Dev BS Loss: {BS_loss}')\n",
        "    print(f'Dev Goal Loss: {goal_loss}')\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  pass\n",
        "correct, all, goal_loss, BS_loss = evaluate(model, hd_test, criterion_BS)\n",
        "print(f'Test Acc: {correct / all: .3f}')\n",
        "print(f'Test BS Loss: {BS_loss}')\n",
        "print(f'Test Goal Loss: {goal_loss}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5BmNWM0W2NA"
      },
      "source": [
        "PStrength_df = pd.read_json('Vec2_25Players/Player_Strength.json', encoding='latin-1', orient='index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tySpHHCUW2NA",
        "outputId": "aa846074-96de-431a-d6f2-07aff8a82762"
      },
      "source": [
        "home_p_strength_list[0].device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EA4Wi3eW2NB",
        "outputId": "80ed9186-926f-4063-bcc1-a600beac59a6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    8.005203\n",
              "1    7.982786\n",
              "Name: Adam Johnson, dtype: float64"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBiZy7EEW2NC",
        "outputId": "b7b4b2a8-ab77-4def-e120-75cabd4f0231"
      },
      "source": [
        "x = torch.stack(home_p_strength_list)[:,-2,:][:,0]\n",
        "y = torch.stack(home_p_strength_list)[:,-2,:][:,1]\n",
        "\n",
        "z = PStrength_df.loc['Kemy Agustien'].to_numpy()\n",
        "w = model.embed(torch.tensor(entities['Kemy Agustien']).to(Globals.DEVICE.value)).detach().cpu()\n",
        "n = range(torch.stack(home_p_strength_list)[:,-2,:][:,0].shape[0])\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x, y, s=80)\n",
        "ax.scatter(z[0], z[1])\n",
        "ax.scatter(w[0], w[1])\n",
        "\n",
        "# z2 = PStrength_df.loc['Ricardo Fuller'].to_numpy()\n",
        "# w2 = model.embed(torch.tensor(entities['Ricardo Fuller']).to(Globals.DEVICE.value)).detach().cpu()\n",
        "# x2 = torch.stack(home_p_strength_list)[:,2,:][:,0]\n",
        "# y2 = torch.stack(home_p_strength_list)[:,2,:][:,1]\n",
        "# ax.scatter(x2, y2, s=80)\n",
        "# ax.scatter(z2[0], z2[1])\n",
        "# ax.scatter(w2[0], w2[1])\n",
        "\n",
        "\n",
        "z2 = PStrength_df.loc['Davide Santon'].to_numpy()\n",
        "w2 = model.embed(torch.tensor(entities['Davide Santon']).to(Globals.DEVICE.value)).detach().cpu()\n",
        "x2 = torch.stack(home_p_strength_list)[:,-1,:][:,0]\n",
        "y2 = torch.stack(home_p_strength_list)[:,-1,:][:,1]\n",
        "ax.scatter(x2, y2, s=80)\n",
        "ax.scatter(z2[0], z2[1])\n",
        "ax.scatter(w2[0], w2[1])\n",
        "\n",
        "fig.set_size_inches(20,10)\n",
        "for i, txt in enumerate(n):\n",
        "    ax.annotate(txt, (x[i], y[i]), fontsize= 12)\n",
        "for i, txt in enumerate(n):\n",
        "    ax.annotate(txt, (x2[i], y2[i]), fontsize= 12)\n",
        "ax.annotate('z', (z[0], z[1]), fontsize= 12)\n",
        "ax.annotate('w', (w[0], w[1]), fontsize= 12)\n",
        "ax.annotate('z2', (z2[0], z2[1]), fontsize= 12)\n",
        "ax.annotate('w2', (w2[0], w2[1]), fontsize= 12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(tensor(-1.1556), tensor(0.3949), 'w2')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6em2mItW2ND"
      },
      "source": [
        "torch.stack(home_p_strength_list)[:,1,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbLI5tiPW2NE",
        "outputId": "2d0a7fb7-619d-46a3-98ac-5c5849c49b17"
      },
      "source": [
        "entities['Samir Nasri']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f77WGRw7W2NF",
        "outputId": "3e8cbf8b-0483-4572-9354-337f791bfd12"
      },
      "source": [
        "model.embed(torch.tensor([166]).to(Globals.DEVICE.value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.0101,  0.1761]], device='cuda:0', grad_fn=<EmbeddingBackward>)"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YtGXFoXW2NF"
      },
      "source": [
        "home_p_dict_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS6M692MW2NG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
